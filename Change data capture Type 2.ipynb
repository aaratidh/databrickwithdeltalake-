{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcdbc5c-a99f-4846-b7bb-24658658c98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- active: long (nullable = true)\n |-- admin2: string (nullable = true)\n |-- combined_key: string (nullable = true)\n |-- confirmed: long (nullable = true)\n |-- country_region: string (nullable = true)\n |-- deaths: long (nullable = true)\n |-- fips: string (nullable = true)\n |-- last_update: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- province_state: string (nullable = true)\n |-- recovered: long (nullable = true)\n\n+------+------+--------------------+---------+--------------+------+----+-------------------+--------+---------+--------------+---------+\n|active|admin2|        combined_key|confirmed|country_region|deaths|fips|        last_update|latitude|longitude|province_state|recovered|\n+------+------+--------------------+---------+--------------+------+----+-------------------+--------+---------+--------------+---------+\n|  null|      |        Anhui, China|        1|         China|  null|    |2020-01-22T17:00:00|  31.826|  117.226|         Anhui|     null|\n|  null|      |      Beijing, China|       14|         China|  null|    |2020-01-22T17:00:00|  40.182|  116.414|       Beijing|     null|\n|  null|      |    Chongqing, China|        6|         China|  null|    |2020-01-22T17:00:00|  30.057|  107.874|     Chongqing|     null|\n|  null|      |       Fujian, China|        1|         China|  null|    |2020-01-22T17:00:00|  26.079|  117.987|        Fujian|     null|\n|  null|      |        Gansu, China|     null|         China|  null|    |2020-01-22T17:00:00|  36.061|  103.834|         Gansu|     null|\n|  null|      |    Guangdong, China|       26|         China|  null|    |2020-01-22T17:00:00|  23.342|  113.424|     Guangdong|     null|\n|  null|      |      Guangxi, China|        2|         China|  null|    |2020-01-22T17:00:00|   23.83|  108.788|       Guangxi|     null|\n|  null|      |      Guizhou, China|        1|         China|  null|    |2020-01-22T17:00:00|  26.815|  106.875|       Guizhou|     null|\n|  null|      |          Hai, China|        4|         China|  null|    |2020-01-22T17:00:00|  19.196|  109.745|           Hai|     null|\n|  null|      |        Hebei, China|        1|         China|  null|    |2020-01-22T17:00:00|  38.043|  114.515|         Hebei|     null|\n|  null|      | Heilongjiang, China|     null|         China|  null|    |2020-01-22T17:00:00|  47.862|  127.761|  Heilongjiang|     null|\n|  null|      |           He, China|        5|         China|  null|    |2020-01-22T17:00:00|  33.882|  113.614|            He|     null|\n|  null|      |Hong Kong, Hong Kong|     null|     Hong Kong|  null|    |2020-01-22T17:00:00|    22.3|    114.2|     Hong Kong|     null|\n|  null|      |        Hubei, China|      444|         China|    17|    |2020-01-22T17:00:00|  30.976|  112.271|         Hubei|       28|\n|  null|      |           Hu, China|        4|         China|  null|    |2020-01-22T17:00:00|   27.61|  111.709|            Hu|     null|\n|  null|      |Inner Mongolia, C...|     null|         China|  null|    |2020-01-22T17:00:00|  44.093|  113.945|Inner Mongolia|     null|\n|  null|      |      Jiangsu, China|        1|         China|  null|    |2020-01-22T17:00:00|  32.971|  119.455|       Jiangsu|     null|\n|  null|      |      Jiangxi, China|        2|         China|  null|    |2020-01-22T17:00:00|  27.614|  115.722|       Jiangxi|     null|\n|  null|      |        Jilin, China|     null|         China|  null|    |2020-01-22T17:00:00|  43.666|  126.192|         Jilin|     null|\n|  null|      |     Liaoning, China|        2|         China|  null|    |2020-01-22T17:00:00|  41.296|  122.609|      Liaoning|     null|\n+------+------+--------------------+---------+--------------+------+----+-------------------+--------+---------+--------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# adding file to aws ()\n",
    "# conneting to aws \n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA5G2VGFGTK7MIPNNH\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "df = spark.read.json(\"s3://databrickproject88/enigma.json\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf3f5793-819d-42d3-bb84-d8de37535e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parse and clean JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f9c7259-55f6-4ed3-bf32-a34461eafdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a surogate key \"row_hash\" and audit_metrics for delta table to capture change in delta table\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, md5, concat_ws\n",
    "\n",
    "df = df.withColumn(\"row_hash\", md5(concat_ws(\"||\", \"province_state\", \"country_region\", \"latitude\", \"longitude\"))) \\\n",
    "       .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "       .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "       .withColumn(\"is_current\", lit(True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ebf03a-e808-4a51-8efc-6349285940d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_path = \"s3://databrickproject88/delta/enigma_cdc/\"\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    delta_tbl = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Step 1: Mark previous rows as not current\n",
    "    delta_tbl.alias(\"t\").merge(\n",
    "        df.alias(\"s\"),\n",
    "        \"t.row_hash = s.row_hash AND t.is_current = true\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"valid_to\": current_timestamp(),\n",
    "        \"is_current\": lit(False)\n",
    "    }).execute()\n",
    "\n",
    "    # Step 2: Insert changed/new rows\n",
    "    delta_tbl.alias(\"t\").merge(\n",
    "        df.alias(\"s\"),\n",
    "        \"t.row_hash = s.row_hash AND t.is_current = false\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "else:\n",
    "    # First time: create table\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5bc3bf9-1b24-4b3c-8ece-571b810ae34e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn(\"ingestion_date\", to_date(current_timestamp()))\n",
    "\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .save(delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af10368b-889b-4704-a58e-4cd66247fd5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43fdb176-4122-48fb-ac8b-63ea969c67d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Change data capture Type 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}